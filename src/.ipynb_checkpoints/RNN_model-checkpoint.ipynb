{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug prediction with LSTM Recurrent Neural Networks\n",
    "In this notebook we explore the possiblity of utilizing a sequential model to detect logical bugs in source code. We are using a similar technique in Nautrual Language Processing (NLP) to sentiment analysis to analize a piece of code. The idea behind this approach is fairly simple, we look at a small snippet of code to determine whether the code is buggy or non-buggy.\n",
    "\n",
    "This notebook will cover some machine learning topics such as recurrent neural networks and long short-term memory units (LSTMs). We also breifly discuss techniques for padding squences in tensorflow. \n",
    "\n",
    "# Tokenizing Source Code\n",
    "\n",
    "In this project we offer a simple Java tokenizer that will replace any Java syntax with a token. We do this for a few reasons, one reason is beacuse it helps split the code apart and we don't have to rely on whitespace to divide the words. For example, it ensures the line \"area=5;\" will split into 4 seperate values instead of generating 1 embedding for the entire line. Another reason we did opposed to removing all syntax is it will give the model more information about how words are related to eachother. The hope is the model will be able to more easily learn logic patterns, for example, if you took \"foo += bar;\" and only extracted the words and removed all the syntax the model would have no idea the example is adding bar to foo. Having a this tokenizer gives the model model information about the code.\n",
    "\n",
    "![tokenizing.png](imgs/tokenizing.png)\n",
    "\n",
    "We are also removing all comments because they can contain English and we only want to show the network code. So we remove all comments using a regular expression.\n",
    "\n",
    "Creating general tokens for strings, would be a good idea since the actual contants of the strings shouldn't matter. Doing the same for intergers and floats might be a good idea as well.\n",
    "\n",
    "# Word Embeddings for source code\n",
    "The approach we're using to analize code is by creating word embeddings for each unique word and token. To understand why we use word embeddings, we need to think about how neural networks work. For a neural network to run efficently the dot products of matrices are often used to make calculations. So, the network will be expecting scalar values, or a vector of scalar values, so it's obvious we will not be able to just feed string values into the model. We need a way to represent strings with scalar values. One approach can be to use one-hot representations to prepair the data, however this medthod doesn't give the model much information about what the word actually means.\n",
    "\n",
    "For better results we will used word embeddings, which require a machine learning method to generate, namely using a skip-gram model. Put simiply Word2Vec will process the corpus, looking at one block of text at a time to train a neural network to guess the current word given some amount of previous words. But we do not need to worry about that as we are using a premade Word2Vec model provided by SciKit Learn. \n",
    "\n",
    "(The embeddings for this project are being generated in the json_to_vector.py file of this project.)\n",
    "\n",
    "![bugAnalysis2.png](imgs/bugAnalysis2.png)\n",
    "\n",
    "Once we have word embeddings we're going to store all the embeddings in a 2-D array. This embedding matrix will be loaded into the neural network and will act as a dictionary for the model to look up the meaning of a word. This matrix will be of size (n, D) where n equals the number of words in the embedding matrix, and D is equal to the size of the embeddings.\n",
    "\n",
    "The vectors are generated in such a way that each dimension represents a feature and this vector will give the machine some context into what the word means and how it relates to other words.\n",
    "\n",
    "![Figure_1000.png](imgs/Figure_1000.png)\n",
    "\n",
    "You can see word that have a similar meaning are grouped together, in the example above we can see trig fuctions are grouped next to eachother as you would expect. Before we create these embeddings we are going to tokenize the source code and I will explain how and why we are doing this. The model is taking a large corpus of source code and outputs vectors for each unique word and token. We store the output into an embedding matrix.\n",
    "\n",
    "![code2emb.png](imgs/code2emb.png)\n",
    "\n",
    "\n",
    "# Generating Buggy Datasets\n",
    "\n",
    "In this project we also take a look at generating buggy datasets for training. Searching for specific tokens we can search through a corpus of code for basic patterns and create basic bugs. Right now we are only generating bugs with swapped arguments. Meaning, we take a line of that has a function call with 2 arguments and simply swap the arugments so that we have an example of code that is likely working and a piece of code that is buggy.\n",
    "\n",
    "![bugGeneration.png](imgs/bugGeneration.png)\n",
    "\n",
    "We are only using examples of swapped arguments so that will be the only bug this model will be able to detct. But this model can be scaled quite easily, all you would have to do is look into generating different logical bugs so the model can learn a wider variety of bugs. \n",
    "\n",
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Now that we have everything set up we are ready to jump into deep learning! In Natural Language Processing (NLP) there is a squential aspect to the data, meaning the order of the words is important. Similarily in code what comes before or after a word/syntax is very important. In order to keep track of the order of input we must utilize a squence model, a recurrent neural network.\n",
    "\n",
    "The structure of a recurrent neural network is different from a simple feedfoward neural network. In a traditional NN input is taken in all at once, the input also has a fixed size. They look something like this...\n",
    "\n",
    "![TraditionalNN.png](imgs/NN.png)\n",
    "\n",
    "The main difference in a RNN is that we now take input in a squential fasion. Each word in the input is now associated with a specific time step. \n",
    "\n",
    "![RNN.png](imgs/RNN.png)\n",
    "\n",
    "At each time step we will calculate activation values that will be passed along to the next time step, often referred to as hidden states. These hidden states will contain information about what the model has already seen in previous time steps. At each step the current hidden state, $a^{<t>}$ is calculated using the current input, $x^{<t>}$, and the previsous hidden state, $a^{<t-1>}$. There is also a bias being added $b_{a}$. The 'g' below refers to an activation function which is typically tanh, and sometimes sigmoid.\n",
    "\n",
    "![HiddenState.png](imgs/HiddenState.png)\n",
    "\n",
    "The 2 W terms refer to the weight matrices. Note that the subscripts are different, it is a common notation standard to label the weight matrices with a superscript or subscript that informs you where to use that matrix. Specifically, for the $W_{ax}$ matrix, the subscript ax tells us this martix is used when calulating $a^{<t>}$ from $x^{<t>}$, and we should take the dot product of the two. The same logic applies to the $W_{aa}$ matrix. Both of the weight matrices are shared at each time step.\n",
    "\n",
    "At the last time step we take the dot product of the activation values and a third $W_{ya}$ matrix and feed the value to a sigmoid function to get a descrete output, between 0 and 1, representing how confident the model is the input was a buggy piece of code.\n",
    "\n",
    "The weight matrices are updated through an optimization process called backpropagation through time. This notebook will not cover backprop in depth, but you need some loss function which will measure how far off the desired output is from the actual output, since this is a classification problem we are using a binary cross entropy cost. In order to minimize the loss function you need to take the derivatives of the loss function with respect to the wieghts and biases. These gradients are used to update the parameters. To be picky this specifically is describing Gradient Descent, backprop is an effictive way of doing Gradient Descent.\n",
    "\n",
    "![GradientDescent.png](imgs/GradientDescent.png)\n",
    "\n",
    "\n",
    "# Exploding and Vanishing Gradients\n",
    "\n",
    "When we are calculating the gradients to update the parameters we go backwards through the model and travel backwards through the computation graph, calulating gradients as we go. So the value of the gradients depends on the gradients calculated earlier in backprop. Using the chain rule these gradients are multiplied together and you might end up with an exponential function.\n",
    "\n",
    "This means if the gradients are slightly over or below 1 the gradients can grow or shrink exponentionally, making them unstable. The weights are updated proportionally to the gradients so if the gradients are too big the weights will be updated dramatically, similarly if they are too small the weights will barely move, and the model will have a very hard time converging. This is a problem in deep neural networks and especially RNN's as the sequence length can get quite long. What ends up happening in basic recurrent neural networks is the model will not be able to reliably remember information more than a couple time steps away.\n",
    "\n",
    "To help with this we are using more complicated recurrent units, long short-term memory units. They help to send the gradients backwards without becoming unstable.\n",
    "\n",
    "There are also ways to strategically initialize the weights too help with this. One technique is called Xavier Initialization, which is done in Keras by default.\n",
    "\n",
    "To better understand backpropagation, __[this lecture explains it very clearly.](https://youtu.be/d14TUNcbn1k)__\n",
    "\n",
    "# Long Short-Term Memory Units (LSTMs)\n",
    "\n",
    "To help the model with long range dependencies we will be using a more advanced recurrent unit. The biggest diffences in LSTM units is they now have memory cells and logic gates for updating these cells with the most relevent information. Now at each time step, we use the previous memory cells, denoted $c^{<t-1>}$, the previous hidden state, $a^{<t-1>}$, and the current input $x^{<t>}$ to calculate new memory cells. There are 3 logic gates for which take the previous hidden state and current input.\n",
    "\n",
    "The equations and diagram is shown below.\n",
    "\n",
    "![LSTM.png](imgs/LSTM.png)\n",
    "\n",
    "You can see at each time step we are proposing a new memory cell, $ẽ^{<t>}$*. The subscripted Gamma's are the logic gates, the first one, subscripted u, is the update gate, it determines how much of the proposed memery cell should be used. The next gate is the forget gate, it determines how much of the previous memory cell you should forget. And the output gate determines the hidden state. These gates are being passed through a sigmoid which will squish values between 0 and 1. If you think about the gates as being either 0 or 1 you can see how easily the equations simplify. For example you can see if the update gate is set to 0 and the forget gate is 1, the equation for the current memory cell simplifies to $c^{<t>}$ = $c^{<t-1>}$. Meaning at this time step you should not update the current memory cell with the propossed one and store everything from the previous memory cell.\n",
    "\n",
    "*ẽ should be a c with a tidle over it because I have not found a proper way to accent letters in Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNLSTM, Embedding\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clean.json\") as f:\n",
    "    clean = json.load(f)\n",
    "with open(\"buggy.json\") as f:\n",
    "    buggy = json.load(f)\n",
    "with open(\"py2vec_modelJ.json\") as f:\n",
    "    embs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = np.asarray(clean)\n",
    "buggy = np.asarray(buggy)\n",
    "buggy_labels = np.ones(len(buggy))\n",
    "clean_labels = np.zeros(len(clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "safemax\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = []\n",
    "int_to_word = []\n",
    "word_to_int = {}\n",
    "i = 0\n",
    "for word, emb in embs.items():\n",
    "    embedding_matrix.append(emb)\n",
    "    int_to_word.append(word)\n",
    "    word_to_int[word] = i\n",
    "    i += 1\n",
    "    \n",
    "embedding_matrix.append(np.ones(100))\n",
    "embedding_matrix = np.asarray(embedding_matrix)\n",
    "print(word_to_int['safemax'])\n",
    "print(int_to_word[2])\n",
    "print(np.array_equal(embs['safemax'], embedding_matrix[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate((clean, buggy), axis=0)\n",
    "train_labels = np.concatenate((clean_labels, buggy_labels), axis=0)\n",
    "\n",
    "for i in range(train_data.shape[0]):\n",
    "    string = ''\n",
    "    for j in range(len(train_data[i])):\n",
    "        string += train_data[i][j] + ' '\n",
    "    train_data[i] = string\n",
    "    \n",
    "np.random.seed(3)\n",
    "np.random.shuffle(train_data)\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = train_data[train_data.shape[0]-1000:]\n",
    "test_labels = train_labels[train_labels.shape[0]-1000:]\n",
    "train_data = train_data[:train_data.shape[0]-1000]\n",
    "train_labels = train_labels[:train_labels.shape[0]-1000]\n",
    "\n",
    "num_words = len(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words embedding found 497424\n",
      "Number of words embedding missing 13988\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens = []\n",
    "test_data_tokens = []\n",
    "num_words_missed = 0\n",
    "num_words_found = 0\n",
    "for i in range(train_data.shape[0]):\n",
    "    train_data_tokens.append([])\n",
    "    for word in train_data[i].split():\n",
    "        if word.lower() in embs:\n",
    "            train_data_tokens[i].append(word_to_int[word.lower()])\n",
    "            num_words_found += 1\n",
    "        else:\n",
    "            train_data_tokens[i].append(-1)\n",
    "            num_words_missed += 1\n",
    "for i in range(test_data.shape[0]):\n",
    "    test_data_tokens.append([])\n",
    "    for word in test_data[i].split():\n",
    "        if word.lower() in embs:\n",
    "            test_data_tokens[i].append(word_to_int[word.lower()])\n",
    "            num_words_found += 1\n",
    "        else:\n",
    "            test_data_tokens[i].append(embedding_matrix.shape[0]-1)\n",
    "            num_words_missed += 1\n",
    "print(\"Number of words embedding found %d\" % num_words_found)\n",
    "print(\"Number of words embedding missing %d\" % num_words_missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_tokens[0])\n",
    "print(test_data_tokens[0])\n",
    "print(embedding_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2526, 2529, 4068, 4068, 5340, 994, 3984, 2561, 2905, 4603, 5918, 5626, 5934]\n",
      "_atsignsymbol_ gwtincompatible _divide_ _divide_ doublemath _dispatch_ roundtoint _openparen_ double _comma_ roundingmode _closeparen_ unknown\n"
     ]
    }
   ],
   "source": [
    "print(train_data_tokens[2])\n",
    "int_to_word.append(\"unknown\")\n",
    "def tokens_to_string(tokens):\n",
    "    words = [int_to_word[token] for token in tokens if token != 0]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "print(tokens_to_string(train_data_tokens[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in train_data_tokens + test_data_tokens]\n",
    "num_tokens = np.asarray(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.09547433989306"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9382613968358966"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pad = pad_sequences(train_data_tokens, maxlen=max_tokens,\n",
    "                              padding=pad, truncating=pad)\n",
    "test_data_pad = pad_sequences(test_data_tokens, maxlen=max_tokens,\n",
    "                             padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35282, 54)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,  400, 5404,\n",
       "       3580, 2298, 2561, 3877, 4603, 1649, 5626, 4029,  994, 5934],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/paperspace/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "num_words = len(int_to_word)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                   output_dim=embedding_matrix.shape[1],\n",
    "                   input_length=max_tokens,\n",
    "                   weights=[embedding_matrix],\n",
    "                   trainable=False,\n",
    "                   name='embedding_layer'))\n",
    "#model.add(Embedding(input_dim=num_words,\n",
    "#                   output_dim=100,\n",
    "#                   input_length=max_tokens,\n",
    "#                   name='embedding_layer'))\n",
    "model.add(CuDNNLSTM(16, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNLSTM(8))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizer,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 54, 100)           593500    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 54, 16)            7552      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 54, 16)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 8)                 832       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 601,893\n",
      "Trainable params: 8,393\n",
      "Non-trainable params: 593,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33517 samples, validate on 1765 samples\n",
      "Epoch 1/10\n",
      "33517/33517 [==============================] - 8s 237us/step - loss: 0.6935 - acc: 0.5052 - val_loss: 0.6898 - val_acc: 0.5439\n",
      "Epoch 2/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.6155 - acc: 0.6462 - val_loss: 0.5089 - val_acc: 0.7275\n",
      "Epoch 3/10\n",
      "33517/33517 [==============================] - 6s 172us/step - loss: 0.4859 - acc: 0.7485 - val_loss: 0.4309 - val_acc: 0.7841\n",
      "Epoch 4/10\n",
      "33517/33517 [==============================] - 6s 170us/step - loss: 0.4358 - acc: 0.7776 - val_loss: 0.3932 - val_acc: 0.7977\n",
      "Epoch 5/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.4088 - acc: 0.7922 - val_loss: 0.3757 - val_acc: 0.8062\n",
      "Epoch 6/10\n",
      "33517/33517 [==============================] - 6s 166us/step - loss: 0.3913 - acc: 0.7984 - val_loss: 0.3649 - val_acc: 0.8125\n",
      "Epoch 7/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.3741 - acc: 0.8056 - val_loss: 0.3518 - val_acc: 0.8113\n",
      "Epoch 8/10\n",
      "33517/33517 [==============================] - 6s 169us/step - loss: 0.3615 - acc: 0.8129 - val_loss: 0.3363 - val_acc: 0.8278\n",
      "Epoch 9/10\n",
      "33517/33517 [==============================] - 6s 168us/step - loss: 0.3538 - acc: 0.8152 - val_loss: 0.3344 - val_acc: 0.8204\n",
      "Epoch 10/10\n",
      "33517/33517 [==============================] - 6s 169us/step - loss: 0.3416 - acc: 0.8224 - val_loss: 0.3223 - val_acc: 0.8300\n",
      "CPU times: user 1min 6s, sys: 7.63 s, total: 1min 14s\n",
      "Wall time: 59.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fead2842cf8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_data_pad, train_labels,\n",
    "         validation_split=0.05, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 152us/step\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data_pad, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 80.10%\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on information from:\n",
    "\n",
    "__[Deep Learning to Find Bugs](http://mp.binaervarianz.de/DeepBugs_TR_Nov2017.pdf)__\n",
    "\n",
    "Idea for automated bug generation.\n",
    "\n",
    "__[Hvass-Labs TensorFlow Tutorial - Natural Language Processing](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/20_Natural_Language_Processing.ipynb)__\n",
    "\n",
    "Tutorial for Keras with TensorFlow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
